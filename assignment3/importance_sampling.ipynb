{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import utils\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn.modules import upsampling\n",
    "from torch.functional import F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset_location, batch_size):\n",
    "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
    "    # start processing\n",
    "    def lines_to_np_array(lines):\n",
    "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
    "    splitdata = []\n",
    "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
    "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
    "        filepath = os.path.join(dataset_location, filename)\n",
    "        utils.download_url(URL + filename, dataset_location)\n",
    "        with open(filepath) as f:\n",
    "            lines = f.readlines()\n",
    "        x = lines_to_np_array(lines).astype('float32')\n",
    "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
    "        # pytorch data loader\n",
    "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
    "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
    "        splitdata.append(dataset_loader)\n",
    "    return splitdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: binarized_mnist/binarized_mnist_train.amat\n",
      "Using downloaded and verified file: binarized_mnist/binarized_mnist_valid.amat\n",
      "Using downloaded and verified file: binarized_mnist/binarized_mnist_test.amat\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = get_data_loader(\"binarized_mnist\", 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(64, 256, 5),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(256, 2 * latent_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        convolved = self.conv_stack(x)\n",
    "        flattened = convolved.view(x.size(0), -1)\n",
    "        z_mean, z_logvar = self.mlp(flattened).chunk(2, dim=-1)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.upsample_stack = nn.Sequential(\n",
    "            nn.Conv2d(256, 64, 5, padding=(4, 4)),\n",
    "            nn.ELU(),\n",
    "            upsampling.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, padding=(2, 2)),\n",
    "            nn.ELU(),\n",
    "            upsampling.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(32, 16, 3, padding=(2, 2)),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(16, 1, 3, padding=(2, 2)),\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        flattened = self.mlp(z)\n",
    "        convolved = self.upsample_stack(flattened.view(z.size(0), 256, 1, 1))\n",
    "        return convolved - 5.\n",
    "    \n",
    "\n",
    "def kl_divergence(mean, logvar, prior_mean, prior_logvar):\n",
    "    output = 0.5 * torch.sum(\n",
    "        prior_logvar - logvar +\n",
    "        ((torch.exp(logvar) + (mean - prior_mean)**2) /\n",
    "         torch.exp(prior_logvar)) - 1., dim=-1\n",
    "    )\n",
    "    return output\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encode = Encoder(latent_size)\n",
    "        self.decode = Decoder(latent_size)\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encode(x)\n",
    "        z_sample = z_mean + torch.exp(z_logvar / 2.) * torch.randn_like(z_logvar)\n",
    "        x_mean = self.decode(z_sample)\n",
    "        return z_mean, z_logvar, x_mean\n",
    "    def loss(self, x, z_mean, z_logvar, x_mean):\n",
    "        ZERO = torch.from_numpy(np.array(0.))\n",
    "        kl = kl_divergence(z_mean, z_logvar, ZERO, ZERO).mean()\n",
    "        recon_loss = F.binary_cross_entropy_with_logits(\n",
    "            x_mean.view(x.size(0), -1),\n",
    "            x.view(x.size(0), -1),\n",
    "            reduction='none'\n",
    "        ).sum(1).mean()\n",
    "        return recon_loss + kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae = VAE(100)\n",
    "params = vae.parameters()\n",
    "optimizer = Adam(params, lr=3e-4)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    for x in train:\n",
    "        z_mean, z_logvar, x_mean = vae(x)\n",
    "        loss = vae.loss(x, z_mean, z_logvar, x_mean)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        total_count = 0\n",
    "        for x in valid:\n",
    "            total_loss += vae.loss(x, *vae(x)) * x.size(0)\n",
    "            total_count += x.size(0)\n",
    "        print(total_loss / total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  next(iter(valid))\n",
    "vae = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = - 0.5 * np.log(2*np.pi)\n",
    "\n",
    "def log_normal(x, mean=None, logvar=None):\n",
    "    # Log prob of scalar gaussian\n",
    "    if mean is None:\n",
    "        mean = 0. #torch.zeros_like(x)\n",
    "    if logvar is None:\n",
    "        logvar = torch.from_numpy(np.array(0.))\n",
    "    sqr_dist = (x - mean)**2\n",
    "    var = torch.exp(logvar)\n",
    "    return -sqr_dist / (2. * var) - logvar/2. + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/.local/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    K = 200\n",
    "\n",
    "    # Sample\n",
    "    z_mean, z_logvar = vae.encode(x)\n",
    "    eps = torch.randn(z_mean.size(0), K, z_mean.size(1))\n",
    "    # Broadcast the noise over the mean and variance\n",
    "    z_samples = z_mean[:, None, :] + torch.exp(z_logvar / 2.)[:, None, :] * eps\n",
    "\n",
    "    # Decode samples\n",
    "\n",
    "    # Flatten out the z samples\n",
    "    z_samples_flat = z_samples.view(-1, z_samples.size(-1)) \n",
    "    x_mean = vae.decode(z_samples_flat) # Push it through\n",
    "    # Bring it back to the original shape\n",
    "    x_mean = x_mean.view(x.size(0), K, x_mean.size(-3), x_mean.size(-2), x_mean.size(-1))\n",
    "\n",
    "    # Probabilities\n",
    "\n",
    "    # Repeat images so they're the same shape as the reconstruction\n",
    "    x_flat = x[:, None].repeat(1, K, 1, 1, 1)\n",
    "\n",
    "    # Calculate all the probabilities!\n",
    "    log_p_x_z = -F.binary_cross_entropy_with_logits(x_mean, x_flat, reduction='none').sum(dim=(-1, -2, -3))\n",
    "    log_q_z_x = log_normal(z_samples, z_mean[:, None, :], z_logvar[:, None, :]).sum(-1) # Broadcasting again.\n",
    "    log_p_z = log_normal(z_samples).sum(-1)\n",
    "\n",
    "    # Recombine them.\n",
    "    w = log_p_x_z + log_p_z - log_q_z_x\n",
    "    k, _ = torch.max(w, dim=1, keepdim=True)\n",
    "    arith_mean = torch.log(torch.mean(torch.exp(w - k))) + k[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-101.7955,  -70.1837, -124.5817,  -75.1420,  -94.6216,  -98.7999,\n",
       "        -108.0888,  -75.3675,  -47.6263,  -70.6153,  -99.8721, -101.5540,\n",
       "        -105.6319,  -94.1859,  -79.0731,  -86.6333,  -52.8111,  -76.0696,\n",
       "        -110.7590,  -93.9893,  -78.7240,  -87.4326,  -98.1920,  -94.1972,\n",
       "        -113.9483,  -53.7177,  -96.9957,  -48.2595, -120.0450,  -88.6475,\n",
       "         -46.4143, -114.3459,  -71.4471, -115.1691,  -52.1049,  -92.5448,\n",
       "        -116.2071,  -97.9242,  -57.5351, -112.6738, -101.1358, -113.8887,\n",
       "        -109.4086, -109.7504,  -84.0596, -100.4736, -110.9115, -123.2812,\n",
       "        -119.9335,  -87.7900,  -77.1799,  -46.4592, -115.0935,  -64.8117,\n",
       "         -65.3133,  -84.7353,  -41.6219, -116.8076,  -82.0027,  -76.8602,\n",
       "         -99.6423,  -70.2371, -117.4422, -108.7642])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arith_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
